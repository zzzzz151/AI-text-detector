import time
from typing import Union, List
import pandas as pd
import sys
import importlib.util
import requests
from .logger import log

from AI_text_detector.models import LM_Script, LM_API

# Clear database
#LM_Script.objects.all().delete()
#LM_API.objects.all().delete()

lm = LM_Script()
lm.name = "chatgpt-roberta"
lm.author = "OpenAI"
lm.description = "ChatGPT Roberta by OpenAI\nThis model is trained on the mix of full-text and splitted sentences of answers from Hello-SimpleAI/HC3.\nMore details refer to arxiv: 2301.07597 and Gtihub project Hello-SimpleAI/chatgpt-comparison-detection.\nThe base checkpoint is roberta-base. We train it with all Hello-SimpleAI/HC3 data (without held-out) for 1 epoch."
lm.script = "AI_text_detector/language_models/chatgpt_detector_roberta.py"
lm.save()

lm = LM_Script()
lm.name = "openai-roberta-base"
lm.author = "OpenAI"
lm.description = "RoBERTa base OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa base model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model. This model was released by OpenAI at the same time as OpenAI released the weights of the largest GPT-2 model, the 1.5B parameter version."
lm.script = "AI_text_detector/language_models/openai_base_roberta.py"
#lm.save()

lm = LM_Script()
lm.name = "openai-roberta-large"
lm.author = "OpenAI"
lm.description = "RoBERTa large OpenAI Detector is the GPT-2 output detector model, obtained by fine-tuning a RoBERTa large model with the outputs of the 1.5B-parameter GPT-2 model. The model can be used to predict if text was generated by a GPT-2 model. This model was released by OpenAI at the same time as OpenAI released the weights of the largest GPT-2 model, the 1.5B parameter version."
lm.script = "AI_text_detector/language_models/openai_large_roberta.py"
#lm.save()

class AI2:
    def __init__(self):
        # Load language models
        self.scriptLMs = {}
        for lm_script in LM_Script.objects.all():
            log("Loading LM " + lm_script.name)
            self.loadLM(lm_script.name, lm_script.script)

        log()
        log("Script LMs loaded:")
        for lm_name in self.scriptLMs.keys():
            log(lm_name)

        log()
        log("API LMs loaded:")
        for lm in LM_API.objects.all():
            log(lm.name)
        log()

    def probability_AI_generated_text(self, text:Union[str, List[str]], lm_name):
        if text.strip() == "":
            return None

        if lm_name in self.scriptLMs.keys():
            # the requested LM is of type script
            ret = self.scriptLMs[lm_name].predict(text)
            if type(ret) != list:
                return round(ret*100)
            # At this point, ret is a list
            ret = [round(prob*100) for prob in ret]
            if len(ret) == 1:
                return ret[0]
            elif len(ret) == 0:
                return None
            else:
                return ret
        
        try:
            api_url = LM_API.objects.get(pk=lm_name).API
            response = requests.post(api_url, data = text)
            probability = response.json()["probability_AI_generated"]
            probability *= 100
            probability = round(probability)
            return probability
        except Exception as e:
            return None
        
        # The requested lm_name doesnt exist, return None
        return None
    

    def loadLM(self, lm_name : str, file : str):
        if file[-3:] != ".py":
            file += ".py"
        spec = importlib.util.spec_from_file_location(lm_name, file)
        newLM = importlib.util.module_from_spec(spec)
        sys.modules[lm_name] = newLM
        spec.loader.exec_module(newLM)
        self.scriptLMs[lm_name] = newLM.LM()

    def unloadLM(self, lm_name):
        if lm_name in self.scriptLMs:
            del self.scriptLMs[lm_name]
            return True 
        return False
