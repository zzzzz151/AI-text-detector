from chatgpt_detector_roberta import ChatGPTRobertaDetectorModel

def getProbability(model, text)->list:
    return [round(prob*100) for prob in model(text)]

if __name__ == '__main__':

    print("Load Model")
    model = ChatGPTRobertaDetectorModel()
    
    # the first is an abstract for the paper GAN Goodfellow et al that was generated by chatgpt
    # the second is the true abstract of the same paper
    batched_input_text = ["Generative Adversarial Networks (GANs) are a type of deep learning model that have gained significant attention in recent years for their ability to generate realistic data samples. GANs are composed of two neural networks, a generator and a discriminator, that are trained simultaneously in a competitive manner. The generator network is tasked with generating samples that can fool the discriminator network into thinking they are real, while the discriminator network is trained to distinguish between real and generated data.\nThis paper provides a comprehensive overview of GANs, including their architecture, training procedure, and applications. We discuss the theoretical foundations of GANs, including the concept of adversarial training and the objective functions used to optimize the generator and discriminator networks. We also review recent advancements in GANs, such as conditional GANs and progressive GANs, that have enabled the generation of high-quality images, videos, and other types of data.\nIn addition to discussing the technical aspects of GANs, we also explore their practical applications, including image synthesis, data augmentation, and style transfer. We highlight the potential of GANs for generating synthetic data for training machine learning models, and discuss their implications for privacy and security.\nOverall, this paper provides a comprehensive overview of Generative Adversarial Networks, and their potential for advancing the field of artificial intelligence.",
                  "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples."]    

    probs = getProbability(model, batched_input_text)
    
    for i, text in enumerate(batched_input_text):
        print(f"Input text: {text[:100]}...")
        print(f"Probability of being computer-generated: {probs[i]}")
    
    while True:
        input_text = input("Input text: ")
        if not input_text:
            break
        print(f"Probability of being computer-generated: {getProbability(model, input_text)[0]}")

