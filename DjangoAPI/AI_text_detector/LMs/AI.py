from typing import Union, List

from .language_models.chatgpt_detector_roberta import ChatGPTRobertaDetectorModel
from .language_models.roberta_openai_detector import OpenAIBaseRobertaGPT2DetectorModel, OpenAILargeRobertaGPT2DetectorModel

language_models = {
        "chatGPT": ChatGPTRobertaDetectorModel,
        "openAIBase": OpenAIBaseRobertaGPT2DetectorModel,
        "openAILarge": OpenAILargeRobertaGPT2DetectorModel
    }
class LanguageModel:
    def __init__(self, model:str=None):
        language_model = language_models[model] if model else ChatGPTRobertaDetectorModel
        # Warning: Takes time to load
        self.model = language_model()
        self.model_name = self.model.__class__.__name__

    def probability_AI_generated_text(self, text:Union[str, List[str]]):
        ret = [round(prob*100) for prob in self.model(text)]
        if len(ret) == 1:
            return ret[0]
        if len(ret) == 0:
            return None
        return ret

if __name__ == '__main__':
    def test_while_loop(model: LanguageModel):
        while True:
            input_text = input("Input text: ")
            if not input_text:
                break
            print(f"Probability of being computer-generated: {model.probability_AI_generated_text(input_text)[0]}")
    model = LanguageModel("chatGPT")

    # the first is an abstract for the paper GAN Goodfellow et al that was generated by chatgpt
    # the second is the true abstract of the same paper
    batched_input_text = [
        "Generative Adversarial Networks (GANs) are a type of deep learning model that have gained significant attention in recent years for their ability to generate realistic data samples. GANs are composed of two neural networks, a generator and a discriminator, that are trained simultaneously in a competitive manner. The generator network is tasked with generating samples that can fool the discriminator network into thinking they are real, while the discriminator network is trained to distinguish between real and generated data.\nThis paper provides a comprehensive overview of GANs, including their architecture, training procedure, and applications. We discuss the theoretical foundations of GANs, including the concept of adversarial training and the objective functions used to optimize the generator and discriminator networks. We also review recent advancements in GANs, such as conditional GANs and progressive GANs, that have enabled the generation of high-quality images, videos, and other types of data.\nIn addition to discussing the technical aspects of GANs, we also explore their practical applications, including image synthesis, data augmentation, and style transfer. We highlight the potential of GANs for generating synthetic data for training machine learning models, and discuss their implications for privacy and security.\nOverall, this paper provides a comprehensive overview of Generative Adversarial Networks, and their potential for advancing the field of artificial intelligence.",
        "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples."]

    probs = model.probability_AI_generated_text(batched_input_text)
    for i, text in enumerate(batched_input_text):
        print(f"Input text: {text[:100]}...")
        print(f"Probability of being computer-generated: {probs[i]}")

    test_while_loop(model)